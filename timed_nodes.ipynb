{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 'unspoiled_nodes' and wrote results to 'cleaned_nodes.csv'.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "\n",
    "def clean_unspoiled_data(input_filename, output_filename):\n",
    "    # Read the entire file as a string\n",
    "    with open(input_filename, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    lines = data.split('\\n')\n",
    "\n",
    "    # Weâ€™ll store rows of [Time, Item Name, Location, Coordinates].\n",
    "    rows = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        # We're looking for lines in the format:\n",
    "        # |Time || {{item icon|Item Name}} || Slot || [[Location]] || (x..,y..) ...\n",
    "        if not line.startswith('|'):\n",
    "            continue\n",
    "        \n",
    "        # Split on '||'\n",
    "        parts = [p.strip() for p in line.split('||')]\n",
    "        if len(parts) < 5:\n",
    "            continue\n",
    "\n",
    "        # Extract the fields we care about:\n",
    "        # parts[0] -> time  (remove leading '|')\n",
    "        # parts[1] -> item\n",
    "        # parts[3] -> location\n",
    "        # parts[4] -> coordinate\n",
    "        time = parts[0].lstrip('|').strip()\n",
    "        item = parts[1]\n",
    "        location = parts[3]\n",
    "        coordinate = parts[4]\n",
    "\n",
    "        # Combine entire line to detect questlink if needed\n",
    "        entire_line = ' '.join(parts)\n",
    "\n",
    "        # Skip if 'questlink' is in the line\n",
    "        if re.search(r'questlink', entire_line, re.IGNORECASE):\n",
    "            continue\n",
    "\n",
    "        # Clean up the item name:\n",
    "        # e.g., {{item icon|Broad Beans}} => Broad Beans\n",
    "        item_clean = re.sub(r'\\{\\{.*?\\|([^\\}]+)\\}\\}', r'\\1', item)\n",
    "        # Remove leftover braces, \"Collectable\", \"(Item)\" text, etc.\n",
    "        item_clean = re.sub(r'[{}]|\\(Item\\)|Collectable', '', item_clean).strip()\n",
    "\n",
    "        # Skip if the item name has the word \"cluster\" (case-insensitive)\n",
    "        if re.search(r'cluster', item_clean, re.IGNORECASE):\n",
    "            continue\n",
    "\n",
    "        # Clean up location, e.g. [[Il Mheg]] => Il Mheg\n",
    "        location_clean = re.sub(r'\\[\\[|\\]\\]', '', location).strip()\n",
    "\n",
    "        # Add the row\n",
    "        rows.append([time, item_clean, location_clean, coordinate])\n",
    "\n",
    "    # Write results to a CSV\n",
    "    with open(output_filename, 'w', encoding='utf-8', newline='') as out_csv:\n",
    "        writer = csv.writer(out_csv)\n",
    "        writer.writerow([\"Time\", \"Item Name\", \"Location\", \"Coordinates\"])\n",
    "        writer.writerows(rows)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"unspoiled_nodes\"       # The raw file in the current directory\n",
    "    output_file = \"cleaned_nodes.csv\"    # Your desired output CSV\n",
    "    clean_unspoiled_data(input_file, output_file)\n",
    "    print(f\"Processed '{input_file}' and wrote results to '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV written to 'final_nodes_with_ids.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Load the JSON file with item IDs.\n",
    "with open(\"item_ids.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    item_json = json.load(f)\n",
    "\n",
    "# Build a mapping: lower-case English item name -> item ID\n",
    "item_mapping = {}\n",
    "for item_id, names in item_json.items():\n",
    "    en_name = names.get(\"en\", \"\").strip().lower()\n",
    "    item_mapping[en_name] = item_id\n",
    "\n",
    "# Read in the cleaned nodes CSV.\n",
    "nodes_df = pd.read_csv(\"cleaned_nodes.csv\")\n",
    "\n",
    "# Function to clean item names for matching:\n",
    "# Remove occurrences of \"(Rare)\" (case-insensitive), then lower-case and strip.\n",
    "def clean_item_name(name):\n",
    "    # Remove the substring (Rare) along with any extra spaces\n",
    "    name_clean = re.sub(r'\\s*\\(rare\\)', '', name, flags=re.IGNORECASE)\n",
    "    return name_clean.strip().lower()\n",
    "\n",
    "nodes_df[\"Item Name Clean\"] = nodes_df[\"Item Name\"].apply(clean_item_name)\n",
    "\n",
    "# Function to look up the item ID using the cleaned item name.\n",
    "def get_item_id(row):\n",
    "    name = row[\"Item Name Clean\"]\n",
    "    if name in item_mapping:\n",
    "        return item_mapping[name]\n",
    "    else:\n",
    "        print(f\"Error: No ID found for item '{row['Item Name']}' (cleaned as '{name}').\")\n",
    "        return None\n",
    "\n",
    "# Apply the lookup function to each row.\n",
    "nodes_df[\"ID\"] = nodes_df.apply(get_item_id, axis=1)\n",
    "\n",
    "# Reorder columns to prepend the ID.\n",
    "final_df = nodes_df[[\"ID\", \"Time\", \"Item Name\", \"Location\", \"Coordinates\"]]\n",
    "\n",
    "# Write the final merged CSV.\n",
    "final_df.to_csv(\"final_nodes_with_ids.csv\", index=False)\n",
    "print(\"Merged CSV written to 'final_nodes_with_ids.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Eorzean time (raw): 23:14\n",
      "Threshold Eorzean time (rounded): 00:00\n",
      "\n",
      "Items scheduled in the next 24 hours:\n",
      "        ID        Time           Item Name                    Location  \\\n",
      "42    5350     1:00 AM     Silkworm Cocoon                 East Shroud   \n",
      "105   9519     1:00 AM              Pumice           Eastern La Noscea   \n",
      "104   5121     1:00 AM       Darksteel Ore  Coerthas Central Highlands   \n",
      "17   32999  2:00 AM/PM  Rarefied Larch Sap                The Ruby Sea   \n",
      "16   32997  2:00 AM/PM  Rarefied Larch Log                The Ruby Sea   \n",
      "..     ...         ...                 ...                         ...   \n",
      "120   5146     6:00 PM            Raw Ruby             Lower La Noscea   \n",
      "121   5151     6:00 PM        Raw Sapphire             Lower La Noscea   \n",
      "70    5546     9:00 PM       Trillium Bulb                 East Shroud   \n",
      "69    6209     9:00 PM           Kidragora                 East Shroud   \n",
      "122   5158     9:00 PM         Astral Rock  Coerthas Central Highlands   \n",
      "\n",
      "    Coordinates  \n",
      "42    (x22,y26)  \n",
      "105   (x17,y26)  \n",
      "104   (x27,y19)  \n",
      "17     (x6,y16)  \n",
      "16     (x6,y16)  \n",
      "..          ...  \n",
      "120   (x23,y21)  \n",
      "121   (x23,y21)  \n",
      "70    (x13,y23)  \n",
      "69    (x13,y23)  \n",
      "122   (x23,y23)  \n",
      "\n",
      "[123 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# -----------------------\n",
    "# 1. Compute current Eorzean time in 24-hour format\n",
    "# -----------------------\n",
    "local_epoch = int(time.time() * 1000)\n",
    "epoch = local_epoch * 20.571428571428573\n",
    "minutes = int((epoch / (1000 * 60)) % 60)\n",
    "hours_24 = int((epoch / (1000 * 60 * 60)) % 24)\n",
    "\n",
    "# Format as 24-hour time (e.g. \"13:59\")\n",
    "et_time_str = f\"{hours_24:02d}:{minutes:02d}\"\n",
    "print(\"Current Eorzean time (raw):\", et_time_str)\n",
    "\n",
    "# -----------------------\n",
    "# 2. Round up to the next hour if minutes > 0 (using 24-hour format)\n",
    "# -----------------------\n",
    "current_time = datetime.strptime(et_time_str, \"%H:%M\")\n",
    "if current_time.minute > 0:\n",
    "    threshold_time = current_time.replace(minute=0, second=0, microsecond=0) + timedelta(hours=1)\n",
    "else:\n",
    "    threshold_time = current_time\n",
    "\n",
    "print(\"Threshold Eorzean time (rounded):\", threshold_time.strftime(\"%H:%M\"))\n",
    "\n",
    "# -----------------------\n",
    "# 3. Load and process final_nodes_with_ids.csv\n",
    "# -----------------------\n",
    "df = pd.read_csv(\"final_nodes_with_ids.csv\")\n",
    "\n",
    "# Helper function to robustly parse the \"Time\" column.\n",
    "def parse_time(time_str):\n",
    "    time_str = time_str.strip()\n",
    "    if \"AM/PM\" in time_str:\n",
    "        # Remove the literal \"AM/PM\" substring.\n",
    "        time_str = time_str.replace(\"AM/PM\", \"\").strip()\n",
    "        # Assume that the remaining time is in 24-hour format.\n",
    "        return datetime.strptime(time_str, \"%H:%M\")\n",
    "    elif \"AM\" in time_str or \"PM\" in time_str:\n",
    "        # Parse using the 12-hour clock and then convert to 24-hour format.\n",
    "        dt = datetime.strptime(time_str, \"%I:%M %p\")\n",
    "        return datetime.strptime(dt.strftime(\"%H:%M\"), \"%H:%M\")\n",
    "    else:\n",
    "        # Already in 24-hour format.\n",
    "        return datetime.strptime(time_str, \"%H:%M\")\n",
    "\n",
    "# Create a new column with parsed times.\n",
    "df[\"Parsed Time\"] = df[\"Time\"].apply(parse_time)\n",
    "\n",
    "# -----------------------\n",
    "# 4. Adjust times for wrapping past midnight and filter for the next 24 hours\n",
    "# -----------------------\n",
    "def effective_time(row_time, threshold):\n",
    "    # If the event time is earlier than the threshold, assume it's on the next day.\n",
    "    return row_time + timedelta(days=1) if row_time < threshold else row_time\n",
    "\n",
    "df[\"Effective Time\"] = df[\"Parsed Time\"].apply(lambda t: effective_time(t, threshold_time))\n",
    "\n",
    "# Define the end of the 24-hour window.\n",
    "end_time = threshold_time + timedelta(days=1)\n",
    "\n",
    "# Filter for events whose effective time is in the next 24 hours.\n",
    "df_filtered = df[(df[\"Effective Time\"] >= threshold_time) & (df[\"Effective Time\"] < end_time)]\n",
    "\n",
    "# Sort by the effective time.\n",
    "df_sorted = df_filtered.sort_values(by=\"Effective Time\")\n",
    "\n",
    "print(\"\\nItems scheduled in the next 24 hours:\")\n",
    "print(df_sorted[[\"ID\", \"Time\", \"Item Name\", \"Location\", \"Coordinates\"]])\n",
    "df_sorted.to_csv(\"final_nodes_with_ids_sorted.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
